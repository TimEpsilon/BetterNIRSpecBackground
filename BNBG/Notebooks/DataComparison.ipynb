{
 "cells": [
  {
   "cell_type": "code",
   "id": "6088c224-a4ca-42cb-97dc-53c6e5bdfa07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:11:01.381277Z",
     "start_time": "2025-03-20T13:11:01.377742Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import ZScaleInterval\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.widgets import Slider\n",
    "from matplotlib.lines import Line2D\n",
    "import astropy.units as u\n",
    "from matplotlib.image import AxesImage\n",
    "from matplotlib.text import Text\n",
    "import matplotlib.patheffects as path_effects\n",
    "import os\n",
    "from astropy.io import fits\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from astropy.table import Table"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "cee81187d18a03ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:11:01.802427Z",
     "start_time": "2025-03-20T13:11:01.785385Z"
    }
   },
   "source": [
    "class DataCube:\n",
    "\t\"\"\"\n",
    "\tAn object combining multiple 1D and 2D spectra from a given folder.\n",
    "\n",
    "\n",
    "\tParameters :\n",
    "\t----------\n",
    "\tfolder :\n",
    "\t\tpath to the folder containing the final data. Will loop on every fits file\n",
    "\t\tand keep the one containing the _s2d or _x1d suffix\n",
    "\n",
    "\tkeyword :\n",
    "\t\tA keyword used for identifying what type of data we're looking at.\n",
    "\n",
    "\tProperties :\n",
    "\t----------\n",
    "\ttable :\n",
    "\t\ta table used for making a correspondence between a source id, a s2d file and a x1d file.\n",
    "\t\tThose files are stored as a list of paths\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, folder, keyword):\n",
    "\n",
    "\t\ts2dList = [file for file in sorted(glob(os.path.join(folder, '*_s2d*')))]\n",
    "\t\tx1dList = [x if os.path.exists(x := file.replace(\"_s2d\", \"_x1d\")) else None for file in s2dList]\n",
    "\t\tsourceList = [fits.open(file)[1].header[\"SOURCEID\"] for file in s2dList]\n",
    "\n",
    "\t\tself.table = pd.DataFrame({\"sourceID\": sourceList, \"s2d\": s2dList, \"x1d\": x1dList, \"keyword\": keyword})\n",
    "\t\tself.table = self.table.groupby(\"sourceID\", sort=False).agg(lambda s : list(s))\n",
    "\n",
    "\t\t# Initializes the dataframe which will contain the data models\n",
    "\t\tself.dataTable = self.table.copy()\n",
    "\n",
    "\n",
    "\tdef combineDataCube(self, datacube):\n",
    "\t\t\"\"\"\n",
    "\t\tCombines 2 Datacubes.\n",
    "\n",
    "\t\tParameters :\n",
    "\t\t----------\n",
    "\t\tdatacube : DataCube\n",
    "\t\t\tAnother datacube to be appended to this one, or None,\n",
    "\t\t\twhich will fill append a none to every list in the array.\n",
    "\n",
    "\t\tReturns :\n",
    "\t\t---------\n",
    "\t\tdc : DataCube\n",
    "\t\t\tThe combined dataframe\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tprint(\"Starting Combining Datacubes\")\n",
    "\n",
    "\t\tprint(\"Merging...\")\n",
    "\t\t# Perform an outer join to include all sourceIDs from both DataFrames\n",
    "\t\tmerged = pd.merge(\n",
    "\t\t\tself.table, datacube.table, on=\"sourceID\", how=\"outer\", suffixes=(\"_self\", \"_other\"), sort=False\n",
    "\t\t)\n",
    "\t\tmerged = merged.map(lambda x: [] if (not isinstance(x, list) and pd.isna(x)) else x)\n",
    "\n",
    "\t\tprint(\"Appending paths...\")\n",
    "\t\t# Combine the 's2d' and 'x1d' columns\n",
    "\t\tmerged[\"s2d\"] = merged[\"s2d_self\"] + merged[\"s2d_other\"]\n",
    "\t\tmerged[\"x1d\"] = merged[\"x1d_self\"] + merged[\"x1d_other\"]\n",
    "\t\tmerged[\"keyword\"] = merged[\"keyword_self\"] + merged[\"keyword_other\"]\n",
    "\t\tmerged.reset_index(drop=False, inplace=True)\n",
    "\n",
    "\t\t# Keep only necessary columns: 'sourceID', 's2d', 'x1d'\n",
    "\t\tself.table = merged[[\"sourceID\", \"s2d\", \"x1d\", \"keyword\"]]\n",
    "\n",
    "\t\tprint(\"Finished Combining Datacubes!\")\n",
    "\n",
    "\tdef preloadDataCube(self):\n",
    "\t\t\"\"\"\n",
    "\t\tInitializes self.dataTable, a table structurally identical to self.table,\n",
    "\t\texcept the paths are replaced by the corresponding datamodels\n",
    "\t\t\"\"\"\n",
    "\t\tprint(\"Starting loading data...\")\n",
    "\t\tprint(\"Copying...\")\n",
    "\t\tself.dataTable = self.table.copy()\n",
    "\n",
    "\t\tprint(\"Loading...\")\n",
    "\t\t# Process the 'x1d' and 's2d' columns\n",
    "\t\tself.dataTable[\"x1d\"] = self.dataTable[\"x1d\"].apply(\n",
    "\t\t\tlambda fileList :\n",
    "\t\t\t[Table.read(file, 1) if isinstance(file, str) else None for file in fileList])\n",
    "\t\tself.dataTable[\"s2d\"] = self.dataTable[\"s2d\"].apply(\n",
    "\t\t\tlambda fileList :\n",
    "\t\t\t[fits.open(file) if isinstance(file, str) else None for file in fileList])\n",
    "\n",
    "\t\tprint(\"Getting Extraction\")\n",
    "\t\tself.dataTable[\"extract\"] = self.table[\"x1d\"].apply(\n",
    "\t\t\tlambda fileList :\n",
    "\t\t\t[fits.open(x1d)[1] for x1d in fileList]\n",
    "\t\t)\n",
    "\t\tself.dataTable[\"extract\"] = self.dataTable[\"extract\"].apply(\n",
    "\t\t\tlambda l:\n",
    "\t\t\t[(x1d.header[\"EXTRXSTR\"],\n",
    "\t\t\tx1d.header[\"EXTRXSTP\"],\n",
    "\t\t\tx1d.header[\"EXTRYSTR\"],\n",
    "\t\t\tx1d.header[\"EXTRYSTP\"])\n",
    "\t\t\tfor x1d in l]\n",
    "\t\t)\n",
    "\n",
    "\t\tprint(\"Flux Correction...\")\n",
    "\t\tfor idx,s2ds in enumerate(self.dataTable[\"s2d\"]):\n",
    "\t\t\tfor j in range(len(s2ds)):\n",
    "\t\t\t\ttoJy = 1\n",
    "\t\t\t\ts2d = s2ds[j]\n",
    "\t\t\t\tif s2d is None:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif s2d[1].header[\"BUNIT\"] == \"MJy/sr\" :\n",
    "\t\t\t\t\ttoJy = s2d[1].header[\"PIXAR_SR\"]*1e6\n",
    "\t\t\t\t\ts2d[1].header[\"BUNIT\"] = \"Jy\"\n",
    "\t\t\t\t\t#print(\"MJy/sr\")\n",
    "\t\t\t\tif s2d[1].header[\"BUNIT\"] == \"MJy\" :\n",
    "\t\t\t\t\ttoJy = 1e6\n",
    "\t\t\t\t\ts2d[1].header[\"BUNIT\"] = \"Jy\"\n",
    "\t\t\t\t\t#print(\"MJy\")\n",
    "\t\t\t\tx1d = self.dataTable[\"x1d\"][idx][j]\n",
    "\t\t\t\tx1d[\"FLUX\"] *= toJy\n",
    "\t\t\t\tx1d[\"FLUX_ERROR\"] *= toJy\n",
    "\n",
    "\t\tprint(\"Finished loading data!\")\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef exploreDataCube(dc):\n",
    "\n",
    "\t\tn = 2\n",
    "\t\tfig, axes = plt.subplots(n+1, 1, figsize=(18, 7), gridspec_kw={'height_ratios': [1]*n +  [4*n]})\n",
    "\t\tplt.subplots_adjust(left=0.1, bottom=0.15, right=0.9, top=0.9, hspace=0)\n",
    "\n",
    "\t\tidx = np.random.randint(len(dc.dataTable))\n",
    "\t\tcolors = [\"xkcd:teal\", \"xkcd:violet\", \"xkcd:hot pink\", \"xkcd:yellow\"]\n",
    "\n",
    "\t\t# Vertical line for mouse tracking\n",
    "\t\tvline = Line2D([0, 0], [0, 1], color='r', linestyle='dashed', linewidth=0.5)\n",
    "\t\taxes[-1].add_line(vline)\n",
    "\t\timage_vlines = [axes[i].axvline(x=0, color='r', linestyle='dashed', linewidth=1) for i in range(n)]\n",
    "\n",
    "\t\t# Initialize spectrum plot\n",
    "\t\tspectrum_lines : list[Line2D] = [axes[-1].plot([], [], label=f\"Legend {i}\", color=colors[i])[0] for i in range(n)]\n",
    "\t\terror_lines = [axes[-1].plot([], [], color=colors[i], linewidth=0.7)[0] for i in range(n)]\n",
    "\t\tsrc_text = axes[-1].text(0.05, 0.05, f\"SourceID: 0\", color=\"k\", transform=axes[-1].transAxes, size=15)\n",
    "\n",
    "\t\t# Initialize image plot\n",
    "\t\timg_artists : list[AxesImage] = []\n",
    "\t\ttext_artist : list[Text] = []\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tim = axes[i].imshow(np.zeros((1,1)), aspect='auto', cmap=\"viridis\", origin=\"lower\", interpolation=\"none\")\n",
    "\t\t\timg_artists.append(im)\n",
    "\n",
    "\t\t\ttxt = axes[i].text(0.02, 0.3, f\"legend {i}\", color=\"w\", transform=axes[i].transAxes)\n",
    "\t\t\ttxt.set_path_effects([path_effects.Stroke(linewidth=3, foreground='black'), path_effects.Normal()])\n",
    "\t\t\ttext_artist.append(txt)\n",
    "\n",
    "\t\t# Extraction lines\n",
    "\t\textr_rects = []\n",
    "\t\tfor i in range(n):\n",
    "\t\t\trect = Rectangle((0,0),1,1, edgecolor='r', facecolor='none', lw=1, linestyle='dotted')\n",
    "\t\t\taxes[i].add_patch(rect)\n",
    "\t\t\textr_rects.append(rect)\n",
    "\n",
    "\t\t# Update sourceID\n",
    "\t\tdef update(val):\n",
    "\t\t\tidx = int(slider.val)  # Get the current slider value\n",
    "\t\t\t#c = 10**slider_coeff.val\n",
    "\n",
    "\t\t\ty1,y2 = np.nan,np.nan\n",
    "\n",
    "\t\t\tfor i in range(n):\n",
    "\t\t\t\tlegend = dc.table[\"keyword\"][idx][i]\n",
    "\n",
    "\t\t\t\tif dc.dataTable[\"s2d\"][idx][i] is not None:\n",
    "\t\t\t\t\timg = dc.dataTable[\"s2d\"][idx][i][1].data\n",
    "\t\t\t\t\tz1, z2 = ZScaleInterval().get_limits(img[img>0])\n",
    "\t\t\t\t\timg_artists[i].set_data(img)\n",
    "\t\t\t\t\timg_artists[i].set_clim(z1, z2)\n",
    "\t\t\t\t\timg_artists[i].set_extent((0,img.shape[1],0,img.shape[0]))\n",
    "\n",
    "\t\t\t\t\txy = dc.dataTable[\"extract\"][idx][i]\n",
    "\t\t\t\t\textr_rects[i].set_bounds((xy[0], xy[2], xy[1]-xy[0], xy[3]-xy[2]))\n",
    "\n",
    "\t\t\t\ttext_artist[i].set_text(legend)\n",
    "\n",
    "\t\t\t\tif not dc.dataTable[\"x1d\"][idx][i] is None:\n",
    "\t\t\t\t\twavelength = dc.dataTable[\"x1d\"][idx][i][\"WAVELENGTH\"].copy()\n",
    "\t\t\t\t\tflux = dc.dataTable[\"x1d\"][idx][i][\"FLUX\"].copy()\n",
    "\t\t\t\t\terr = dc.dataTable[\"x1d\"][idx][i][\"FLUX_ERROR\"].copy()\n",
    "\t\t\t\t\tmask = (flux>0)\n",
    "\n",
    "\t\t\t\t\t#if i == 1:\n",
    "\t\t\t\t\t#\tflux *= c\n",
    "\t\t\t\t\t#\terr *= c\n",
    "\n",
    "\t\t\t\t\tspectrum_lines[i].set_data(wavelength[mask], flux[mask])\n",
    "\t\t\t\t\tspectrum_lines[i].set_label(legend)\n",
    "\t\t\t\t\terror_lines[i].set_data(wavelength[mask], err[mask])\n",
    "\n",
    "\t\t\t\t\ty1,y2 = np.nanmin(np.append(flux[mask],y1)), np.nanmax(np.append(flux[mask],y2))\n",
    "\n",
    "\t\t\tsrc_text.set_text(f\"SourceID: {dc.dataTable['sourceID'][idx]}\")\n",
    "\t\t\taxes[-1].legend()\n",
    "\t\t\taxes[-1].relim()\n",
    "\t\t\taxes[-1].set_ylim(y1,y2)\n",
    "\t\t\t\"\"\"\n",
    "\t\t\ty = dc.dataTable[\"x1d\"][idx][0][\"FLUX\"].copy()\n",
    "\t\t\tx = dc.dataTable[\"x1d\"][idx][0][\"WAVELENGTH\"].copy()\n",
    "\t\t\tyi = c*np.interp(x,dc.dataTable[\"x1d\"][idx][1][\"WAVELENGTH\"],dc.dataTable[\"x1d\"][idx][1][\"FLUX\"])\n",
    "\t\t\tdyi = c**2*(np.interp(x,dc.dataTable[\"x1d\"][idx][1][\"WAVELENGTH\"],dc.dataTable[\"x1d\"][idx][1][\"FLUX_ERROR\"])**2\n",
    "\t\t\t\t   + dc.dataTable[\"x1d\"][idx][0][\"FLUX_ERROR\"])**2\n",
    "\t\t\tchi2 = np.nansum((y-yi)**2/dyi)\n",
    "\t\t\tax_chi.scatter(chi2,np.log10(c), color='k', marker=\".\")\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tfig.canvas.draw_idle()\n",
    "\n",
    "\n",
    "\t\tdef onKey(event):\n",
    "\t\t\tcurrent = slider.val\n",
    "\t\t\tif event.key == \"right\":  # Move slider one step right\n",
    "\t\t\t\tnew = min(current + 1, N - 1)  # Ensure within bounds\n",
    "\t\t\t\tslider.set_val(new)\n",
    "\t\t\telif event.key == \"left\":  # Move slider one step left\n",
    "\t\t\t\tnew = max(current - 1, 0)  # Ensure within bounds\n",
    "\t\t\t\tslider.set_val(new)\n",
    "\n",
    "\t\t# Slider\n",
    "\t\tax_slider = plt.axes((0.2, 0.05, 0.6, 0.03))\n",
    "\t\tN = len(dc.dataTable[\"sourceID\"])\n",
    "\t\tslider = Slider(ax_slider, 'Source', 0, N - 1, valinit=idx, valstep=1)\n",
    "\n",
    "\t\t\"\"\"ax_coeff = plt.axes((0.92,0.1,0.0225,0.8))\n",
    "\t\tslider_coeff = Slider(ax_coeff, 'Coefficient', -2, 2, valinit=0, valstep=0.001, orientation=\"vertical\")\n",
    "\t\tax_chi = plt.axes((0.95,0.1,0.045,0.8))\n",
    "\t\tax_chi.set_ylim(-2,2)\n",
    "\t\tax_chi.set_xscale('log')\n",
    "\t\tax_chi.grid(True)\"\"\"\n",
    "\n",
    "\t\tupdate(idx)\n",
    "\n",
    "\t\t# Attach the update function to the slider\n",
    "\t\tslider.on_changed(update)\n",
    "\t\t#slider_coeff.on_changed(update)\n",
    "\n",
    "\t\tdef on_move(event):\n",
    "\t\t\tif event.inaxes == axes[-1]:  # Only update if cursor is in the spectral plot\n",
    "\t\t\t\tidx = slider.val\n",
    "\n",
    "\t\t\t\tcursor_wavelength = event.xdata  # Get the wavelength from the cursor\n",
    "\n",
    "\t\t\t\tif cursor_wavelength is None:\n",
    "\t\t\t\t\treturn\n",
    "\n",
    "\t\t\t\tvline.set_xdata([cursor_wavelength, cursor_wavelength])  # Update main plot\n",
    "\t\t\t\tvline.set_ydata(axes[-1].get_ylim())\n",
    "\t\t\t\tfor i in range(n):\n",
    "\t\t\t\t\tif dc.dataTable[\"s2d\"][idx][i] is not None:\n",
    "\t\t\t\t\t\twavelength_map = np.nanmean(dc.dataTable[\"s2d\"][idx][i][3].data, axis=0)  # Get the wavelength mapping\n",
    "\n",
    "\t\t\t\t\t\t# Find the closest pixel index\n",
    "\t\t\t\t\t\tpixel_pos = np.abs(wavelength_map - cursor_wavelength).argmin()\n",
    "\n",
    "\t\t\t\t\t\t# Update vertical line in the corresponding image subplot\n",
    "\t\t\t\t\t\timage_vlines[i].set_xdata([pixel_pos, pixel_pos])\n",
    "\n",
    "\t\t\t\tfig.canvas.draw_idle()\n",
    "\n",
    "\n",
    "\t\t# Connect keypress handler\n",
    "\t\tfig.canvas.mpl_connect(\"key_press_event\", onKey)\n",
    "\t\tfig.canvas.mpl_connect(\"motion_notify_event\", on_move)\n",
    "\n",
    "\t\taxes[-1].set_xlim(0.5,5.4)\n",
    "\t\taxes[-1].set_xlabel(r\"$\\lambda$ (µm)\")\n",
    "\t\taxes[-1].set_ylabel(r\"Flux (Jy)\")\n",
    "\t\taxes[-1].set_yscale(\"log\")\n",
    "\t\taxes[-1].legend()\n",
    "\t\taxes[-1].grid()\n",
    "\n",
    "\t\t# Show the plot\n",
    "\t\tplt.show(block=False)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6fc88b345de3d5c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:11:05.678797Z",
     "start_time": "2025-03-20T13:11:02.799056Z"
    }
   },
   "source": [
    "dc = DataCube(\"../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P5\", \"P5-Basic\")\n",
    "dc.combineDataCube(DataCube(\"../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3\", \"P3-Basic\"))\n",
    "dc.combineDataCube(DataCube(\"../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P2\", \"P2-Basic\"))\n",
    "dc.combineDataCube(DataCube(\"../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P1\", \"P1-Basic\"))\n",
    "\n",
    "dc.combineDataCube(DataCube(\"../../mastDownload/JWST/CAPERS/P5/Final\", \"P5-BNBG\"))\n",
    "dc.combineDataCube(DataCube(\"../../mastDownload/JWST/CAPERS/P3/Final\", \"P3-BNBG\"))\n",
    "dc.combineDataCube(DataCube(\"../../mastDownload/JWST/CAPERS/P2/Final\", \"P2-BNBG\"))\n",
    "\n",
    "dc.combineDataCube(DataCube(\"../../mastDownload/JWST/CAPERS/P1/Final\", \"P1-BNBG\"))\n",
    "\n",
    "mask = dc.table[\"keyword\"].apply(lambda x : any(\"BNBG\" in item for item in x) and any(\"Basic\" in item for item in x))\n",
    "dc.table = dc.table[mask]\n",
    "dc.table.reset_index(drop=True, inplace=True) # Reset indexes\n",
    "\n",
    "display(dc.table)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Combining Datacubes\n",
      "Merging...\n",
      "Appending paths...\n",
      "Finished Combining Datacubes!\n",
      "Starting Combining Datacubes\n",
      "Merging...\n",
      "Appending paths...\n",
      "Finished Combining Datacubes!\n",
      "Starting Combining Datacubes\n",
      "Merging...\n",
      "Appending paths...\n",
      "Finished Combining Datacubes!\n",
      "Starting Combining Datacubes\n",
      "Merging...\n",
      "Appending paths...\n",
      "Finished Combining Datacubes!\n",
      "Starting Combining Datacubes\n",
      "Merging...\n",
      "Appending paths...\n",
      "Finished Combining Datacubes!\n",
      "Starting Combining Datacubes\n",
      "Merging...\n",
      "Appending paths...\n",
      "Finished Combining Datacubes!\n",
      "Starting Combining Datacubes\n",
      "Merging...\n",
      "Appending paths...\n",
      "Finished Combining Datacubes!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     sourceID                                                s2d  \\\n",
       "0         299  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...   \n",
       "1         501  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...   \n",
       "2        1279  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...   \n",
       "3        1294  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...   \n",
       "4        1432  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...   \n",
       "..        ...                                                ...   \n",
       "665    154203  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P1/CAPER...   \n",
       "666    157001  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P1/CAPER...   \n",
       "667    157420  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P5/CAPER...   \n",
       "668    159638  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...   \n",
       "669    159920  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P2/CAPER...   \n",
       "\n",
       "                                                   x1d              keyword  \n",
       "0    [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...  [P3-Basic, P3-BNBG]  \n",
       "1    [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...  [P3-Basic, P3-BNBG]  \n",
       "2    [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...  [P3-Basic, P3-BNBG]  \n",
       "3    [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...  [P3-Basic, P3-BNBG]  \n",
       "4    [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...  [P3-Basic, P3-BNBG]  \n",
       "..                                                 ...                  ...  \n",
       "665  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P1/CAPER...  [P1-Basic, P1-BNBG]  \n",
       "666  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P1/CAPER...  [P1-Basic, P1-BNBG]  \n",
       "667  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P5/CAPER...  [P5-Basic, P5-BNBG]  \n",
       "668  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...  [P3-Basic, P3-BNBG]  \n",
       "669  [../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P2/CAPER...  [P2-Basic, P2-BNBG]  \n",
       "\n",
       "[670 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sourceID</th>\n",
       "      <th>s2d</th>\n",
       "      <th>x1d</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>299</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[P3-Basic, P3-BNBG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>501</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[P3-Basic, P3-BNBG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1279</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[P3-Basic, P3-BNBG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1294</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[P3-Basic, P3-BNBG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1432</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[P3-Basic, P3-BNBG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>154203</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P1/CAPER...</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P1/CAPER...</td>\n",
       "      <td>[P1-Basic, P1-BNBG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>157001</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P1/CAPER...</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P1/CAPER...</td>\n",
       "      <td>[P1-Basic, P1-BNBG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>157420</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P5/CAPER...</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P5/CAPER...</td>\n",
       "      <td>[P5-Basic, P5-BNBG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>159638</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P3/CAPER...</td>\n",
       "      <td>[P3-Basic, P3-BNBG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>159920</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P2/CAPER...</td>\n",
       "      <td>[../../../CAPERS/V0.1/CAPERS_UDS_V0.1/P2/CAPER...</td>\n",
       "      <td>[P2-Basic, P2-BNBG]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>670 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "b5e18a216b0b9617",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-03-20T13:11:20.979036Z",
     "start_time": "2025-03-20T13:11:05.690462Z"
    }
   },
   "source": "dc.preloadDataCube()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loading data...\n",
      "Copying...\n",
      "Loading...\n",
      "Getting Extraction\n",
      "Flux Correction...\n",
      "Finished loading data!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "2199f9ae9ad08e01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:11:20.992036Z",
     "start_time": "2025-03-20T13:11:20.990177Z"
    }
   },
   "source": "%matplotlib Qt5Agg",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a3ae2de047583bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:11:21.079186Z",
     "start_time": "2025-03-20T13:11:21.036813Z"
    }
   },
   "source": [
    "plt.close(\"all\")\n",
    "DataCube.exploreDataCube(dc)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed3c19219a378902"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
